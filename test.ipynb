{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd79fcf9",
   "metadata": {},
   "source": [
    "## Step - 1 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893e2aa",
   "metadata": {},
   "source": [
    "Computers cant understand words directly, so we map each word to unique number (ID). This process is called Tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "def tokenize(text, vocab):\n",
    "    return [vocab.get(word, vocab[\"<UNK>\"]) for word in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8db872",
   "metadata": {},
   "source": [
    "## Step-2 Embeddings Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ef514",
   "metadata": {},
   "source": [
    "Numbers alone like (0 and 1) dont carry any meaning with themselves. An Embeddings layer transforms this numbers into vectors (list of numbers), allowing words with similar meaning to have similar representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeaa133",
   "metadata": {},
   "source": [
    "## Step-3 Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8b0424",
   "metadata": {},
   "source": [
    "Transformers process all the words at once , so they dont naturally understand (e.g. \"I love You\" != \"You Love I\"). Positional Encoding fixes this by adding a unique \"position signal\" to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_seq_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        pe = torch.zeros(max_seq_len, embedding_dim)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7202b53",
   "metadata": {},
   "source": [
    "## Step-4 Self Attentiom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dede42",
   "metadata": {},
   "source": [
    "Self Attention helps the model to focus on the important words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "942933a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.key = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.value = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / torch.sqrt(torch.tensor(x.size(-1), dtype=torch.float32))\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        attended_values = torch.bmm(attention_weights, values)\n",
    "        return attended_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75edda7",
   "metadata": {},
   "source": [
    "## Step-5 Transformers Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14bfcb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embedding_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attended = self.attention(x)\n",
    "        x = self.norm1(x + attended)\n",
    "        forwarded = self.feed_forward(x)\n",
    "        x = self.norm2(x + forwarded)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c60d74",
   "metadata": {},
   "source": [
    "## Step-6 : Full Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a164b68",
   "metadata": {},
   "source": [
    "Now we will be combining all the models at one model that depicts the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "459649d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(SimpleLLM, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim)\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerBlock(embedding_dim, hidden_dim) for _ in range(num_layers)])\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(0, 1) # Transpose for positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        x = x.transpose(0, 1) # Transpose back\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a632fe06",
   "metadata": {},
   "source": [
    "## Step-7 : Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dde259",
   "metadata": {},
   "source": [
    "Now we will teach the model by showing it examples and correcting its mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ac00c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.1377586126327515\n",
      "Epoch 10, Loss: 0.5685836672782898\n",
      "Epoch 20, Loss: 0.2696878910064697\n",
      "Epoch 30, Loss: 0.16055899858474731\n",
      "Epoch 40, Loss: 0.1057790070772171\n",
      "Epoch 50, Loss: 0.07413160055875778\n",
      "Epoch 60, Loss: 0.05436704307794571\n",
      "Epoch 70, Loss: 0.04131461679935455\n",
      "Epoch 80, Loss: 0.03233238682150841\n",
      "Epoch 90, Loss: 0.025909876450896263\n"
     ]
    }
   ],
   "source": [
    "vocab = {\"hello\": 0, \"world\": 1, \"how\": 2, \"are\": 3, \"you\": 4, \"<UNK>\": 5}\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 16\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "\n",
    "model = SimpleLLM(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "data = [\"hello world how are you\", \"how are you hello world\"]\n",
    "tokenized_data = [tokenize(sentence, vocab) for sentence in data]\n",
    "\n",
    "for epoch in range(100):\n",
    "    for sentence in tokenized_data:\n",
    "        for i in range(1, len(sentence)):\n",
    "            input_seq = torch.tensor(sentence[:i]).unsqueeze(0)\n",
    "            target = torch.tensor(sentence[i]).unsqueeze(0)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "            loss = criterion(output[:, -1, :], target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7880bd93",
   "metadata": {},
   "source": [
    "## Step-8 : Using the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66145195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: hello world, Predicted: how\n"
     ]
    }
   ],
   "source": [
    "input_text = \"hello world\"\n",
    "input_tokens = tokenize(input_text, vocab)\n",
    "input_tensor = torch.tensor(input_tokens).unsqueeze(0)\n",
    "output = model(input_tensor)\n",
    "predicted_token = torch.argmax(output[:, -1, :]).item()\n",
    "print(f\"Input: {input_text}, Predicted: {list(vocab.keys())[list(vocab.values()).index(predicted_token)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00276c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71597300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40319ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
